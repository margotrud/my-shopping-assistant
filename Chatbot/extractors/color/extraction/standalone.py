# Chatbot/extractors/color/extraction/standalone.py
"""
standalone.py
=============

Handles extraction of standalone tone/modifier terms,
including simplified and injected expressions.
"""

from Chatbot.extractors.color.shared.constants import COSMETIC_NOUNS
from Chatbot.extractors.color.utils.config_loader import load_json_from_data_dir
from Chatbot.extractors.color.utils.modifier_resolution import resolve_modifier_token, fuzzy_match_modifier_safe
from Chatbot.extractors.general.utils.fuzzy_match import normalize_token, match_expression_aliases


def extract_standalone_phrases(tokens, known_modifiers, known_tones, debug=False):
    all_terms = _inject_expression_modifiers(tokens, known_modifiers, debug)
    filtered_terms = _extract_filtered_tokens(tokens, known_modifiers, known_tones, debug)
    final = _finalize_standalone_phrases(all_terms, filtered_terms, debug)
    return final


def levenshtein_distance(a, b):
    """Simple Levenshtein approximation: how many characters differ."""
    return sum(1 for x, y in zip(a, b) if x != y) + abs(len(a) - len(b))

def is_suffix_variant(word: str, base: str, debug=False) -> bool:
    if debug:
        print(f"[ğŸ§ª SUFFIX CHECK] word='{word}', base='{base}'")

    if word == base:
        if debug:
            print(f"[âŒ NOT SUFFIX] word == base â†’ no suffix")
        return False

    allowed_suffixes = {"y", "ish"}
    suffix = ""

    if word.startswith(base):
        suffix = word[len(base):]
        if debug:
            print(f"[ğŸ§ª SUFFIX EXTRACTED] word starts with base â†’ suffix='{suffix}'")
        if suffix in allowed_suffixes:
            print(f"[âœ… VALID SUFFIX] '{word}' = '{base}' + '{suffix}'")
            return True
    elif base.endswith("e") and word.startswith(base[:-1]):
        suffix = word[len(base) - 1:]
        if debug:
            print(f"[ğŸ§ª ALT-SUFFIX CHECK] Trying base minus 'e' â†’ suffix='{suffix}'")
        if suffix in allowed_suffixes:
            print(f"[âœ… ALT VALID SUFFIX] '{word}' = '{base[:-1]}' + '{suffix}' (from '{base}')")
            return True

    if debug:
        print(f"[âŒ NOT A SUFFIX VARIANT] (word='{word}', base='{base}')")
    return False


def _inject_expression_modifiers(tokens, known_modifiers, debug):
    """
    Injects modifier terms from matched style expressions and direct token matching.

    - Detects expression aliases using match_expression_aliases().
    - Injects known modifiers tied to expressions (e.g., "romantic" â†’ ["soft", "rosy"]).
    - Also directly adds tokens that match known_modifiers via fuzzy match.

    Args:
        tokens (List[spacy.tokens.Token]): spaCy tokens
        known_modifiers (Set[str]): Known modifiers
        debug (bool): If True, prints debug logs

    Returns:
        Set[str]: Injected modifier terms
    """
    injected = set()
    text = " ".join([t.text for t in tokens]).lower()

    # â”€â”€â”€ Load expressions
    expression_def = load_json_from_data_dir("expression_definition.json")
    matched = match_expression_aliases(text, expression_def)
    if not isinstance(matched, set):
        matched = set()

    if debug:
        print(f"[ğŸ§  MATCHED EXPRESSIONS] {matched}")

    for expr in matched:
        mods = expression_def.get(expr, {}).get("modifiers", [])
        valid = [m for m in mods if m in known_modifiers]
        if debug:
            print(f"[ğŸ¯ INJECTED] '{expr}' â†’ {valid}")
        injected.update(valid)

    # â”€â”€â”€ Direct + fuzzy token resolution
    for tok in tokens:
        word = normalize_token(tok.text)
        if tok.pos_ == "NOUN" and word in COSMETIC_NOUNS:
            if debug:
                print(f"[â›” SKIP] '{word}' (cosmetic noun)")
            continue

        if word in known_modifiers:
            injected.add(word)
            if debug:
                print(f"[âœ… DIRECT] '{word}'")
        else:
            fuzzy = fuzzy_match_modifier_safe(word, known_modifiers)
            if fuzzy:
                injected.add(fuzzy)
                if debug:
                    print(f"[âœ¨ FUZZY] '{word}' â†’ '{fuzzy}'")

    return injected

def _extract_filtered_tokens(tokens, known_modifiers, known_tones, debug):
    result = set()

    for tok in tokens:
        raw = normalize_token(tok.text)

        if debug:
            print(f"\n[ğŸ§ª TOKEN] '{tok.text}' â†’ normalized: '{raw}' (POS={tok.pos_})")
            print(f"[ğŸ” CHECK] In COSMETIC_NOUNS? â†’ {raw in COSMETIC_NOUNS}")

        # âœ… Block known cosmetic nouns
        if raw in COSMETIC_NOUNS:
            if debug:
                print(f"[â›” SKIPPED] Cosmetic noun '{raw}' blocked")
            continue

        # âœ… Skip connector words (and, or, etc.) via POS tag
        if tok.pos_ == "CCONJ":
            if debug:
                print(f"[â›” SKIPPED] Connector '{raw}' ignored (POS=CCONJ)")
            continue

        # âœ… Resolve token
        resolved = resolve_modifier_token(raw, known_modifiers, known_tones)

        if debug:
            print(f"[ğŸ” RESOLVED] '{raw}' â†’ '{resolved}'")
            print(f"[ğŸ“Œ raw âˆˆ tones?] {raw in known_tones}")
            print(f"[ğŸ“Œ resolved âˆˆ tones?] {resolved in known_tones if resolved else 'â€”'}")
            print(f"[ğŸ“ resolved == raw?] {resolved == raw if resolved else 'â€”'}")
            print(f"[ğŸ“ resolved starts with raw?] {resolved.startswith(raw) if resolved else 'â€”'}")
            print(f"[ğŸ“ contains hyphen?] {'-' in resolved if resolved else 'â€”'}")
            print(f"[ğŸ§® total matches so far] {len(result)}")

        # ğŸ”’ Block fuzzy match result if too short to trust
        if len(raw) <= 3 and resolved != raw:
            if debug:
                print(f"[â›” REJECTED] Token '{raw}' too short for safe fuzzy match â†’ '{resolved}'")
            continue

        # ğŸ”’ Reject fuzzy compound result unless raw is root
        if resolved and "-" in resolved and not resolved.startswith(raw):
            if debug:
                print(f"[â›” REJECTED] Fuzzy '{raw}' â†’ '{resolved}' (compound mismatch)")
            continue

        # ğŸ”’ Reject multi-word result from a single token
        if resolved and " " in resolved and " " not in raw:
            if debug:
                print(f"[â›” REJECTED] Fuzzy '{raw}' â†’ '{resolved}' (multi-word result from single token)")
            continue

        # ğŸ”’ If already have strong matches, skip risky fuzzy ones
        if len(result) >= 3 and resolved != raw:
            if debug:
                print(f"[â›” REJECTED] Skipping fuzzy '{raw}' â†’ '{resolved}' (already 3+ matches)")
            continue

        if resolved:
            result.add(resolved)
            if debug:
                print(f"[ğŸ¯ STANDALONE MATCH] '{raw}' â†’ '{resolved}'")

    return result

def _finalize_standalone_phrases(injected, filtered, debug):
    combined = injected.union(filtered)
    if debug:
        print(f"[âœ… FINAL STANDALONE SET] {combined}")
    return combined
def extract_lone_tones(tokens, known_tones, debug=False):
    """
    Extracts standalone tone tokens directly from token stream.

    This version:
    - Uses normalize_token() for cleanup
    - Skips cosmetic product nouns (e.g., 'lipstick', 'blush')
    - Accepts any token directly in known_tones

    Args:
        tokens (List[spacy.tokens.Token]): spaCy tokens
        known_tones (Set[str]): Tone vocabulary
        debug (bool): If True, prints debug info

    Returns:
        Set[str]: Set of matched tone tokens
    """
    matches = set()
    for tok in tokens:
        raw = normalize_token(tok.text)
        if raw in COSMETIC_NOUNS:
            if debug:
                print(f"[â›” COSMETIC BLOCK] '{raw}' blocked")
            continue
        if raw in known_tones:
            matches.add(raw)
            if debug:
                print(f"[ğŸ¯ LONE TONE] Found '{raw}'")
    return matches
